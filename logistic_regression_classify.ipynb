{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c321ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63529aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"./data/iris/iris.data\", header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb564ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4], dtype='int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb4972d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
       "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91538af",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "   1. sepal length in cm\n",
    "   2. sepal width in cm\n",
    "   3. petal length in cm\n",
    "   4. petal width in cm\n",
    "   5. class: \n",
    "      -- Iris Setosa\n",
    "      -- Iris Versicolour\n",
    "      -- Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52aa13a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df[4].isin([\"Iris-setosa\", \"Iris-virginica\"])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfbb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"Iris-setosa\": 1, \"Iris-virginica\": 0}\n",
    "y = df.replace({4: classes})[4]\n",
    "X = df.iloc[:, :4]\n",
    "X['b'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19ed2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: ndarray)->ndarray:\n",
    "    return 1. / (1. + np.exp(-np.clip(z, -255, 255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8074ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesis(theta: ndarray, X: ndarray) -> ndarray:\n",
    "    z =  np.dot(theta, X.T)\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518c07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(theta: ndarray, X: ndarray, y: ndarray)-> float:\n",
    "    return y - thesis(theta, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cee821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(loss_v: ndarray, X: ndarray, theta: ndarray) -> ndarray:\n",
    "    return X.T.dot(loss_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "686af27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: ndarray, W: ndarray, y: ndarray, epochs: int, lr: float) -> ndarray:\n",
    "    for i in range(epochs):\n",
    "        loss_v = loss(W, X, y)\n",
    "        print(f'Current epoch: {i}, Running losses: {sum(loss_v) / len(loss_v)}')\n",
    "        gradient_v = gradient(loss_v, X, W)\n",
    "        W = np.add(W, np.multiply(lr, gradient_v))\n",
    "        print(f'Current epoch: {i}, Running Weights: {W}')\n",
    "        print('-------------')\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4b3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[1]\n",
    "m = X.shape[0]\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d5f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2232\n",
    "rgen = np.random.RandomState(random_state)\n",
    "W = rgen.normal(loc=0.0, scale=0.01, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041188cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0, Running losses: 0.019610748746525478\n",
      "Current epoch: 0, Running Weights: [-0.01017469 -0.01598625  0.0086513   0.00572001 -0.00530658]\n",
      "-------------\n",
      "Current epoch: 1, Running losses: 0.01962306302919102\n",
      "Current epoch: 1, Running Weights: [-0.01017754 -0.0159845   0.00864167  0.00571574 -0.00530638]\n",
      "-------------\n",
      "Current epoch: 2, Running losses: 0.01963537528075951\n",
      "Current epoch: 2, Running Weights: [-0.01018038 -0.01598275  0.00863205  0.00571147 -0.00530619]\n",
      "-------------\n",
      "Current epoch: 3, Running losses: 0.019647685501526527\n",
      "Current epoch: 3, Running Weights: [-0.01018323 -0.01598099  0.00862243  0.00570719 -0.00530599]\n",
      "-------------\n",
      "Current epoch: 4, Running losses: 0.01965999369178807\n",
      "Current epoch: 4, Running Weights: [-0.01018607 -0.01597924  0.00861281  0.00570292 -0.00530579]\n",
      "-------------\n",
      "Current epoch: 5, Running losses: 0.01967229985184016\n",
      "Current epoch: 5, Running Weights: [-0.01018892 -0.01597748  0.00860318  0.00569865 -0.0053056 ]\n",
      "-------------\n",
      "Current epoch: 6, Running losses: 0.019684603981978636\n",
      "Current epoch: 6, Running Weights: [-0.01019176 -0.01597573  0.00859356  0.00569438 -0.0053054 ]\n",
      "-------------\n",
      "Current epoch: 7, Running losses: 0.019696906082499157\n",
      "Current epoch: 7, Running Weights: [-0.0101946  -0.01597397  0.00858394  0.00569011 -0.0053052 ]\n",
      "-------------\n",
      "Current epoch: 8, Running losses: 0.01970920615369755\n",
      "Current epoch: 8, Running Weights: [-0.01019744 -0.01597222  0.00857433  0.00568584 -0.00530501]\n",
      "-------------\n",
      "Current epoch: 9, Running losses: 0.019721504195869426\n",
      "Current epoch: 9, Running Weights: [-0.01020029 -0.01597046  0.00856471  0.00568157 -0.00530481]\n",
      "-------------\n",
      "Current epoch: 10, Running losses: 0.019733800209310683\n",
      "Current epoch: 10, Running Weights: [-0.01020313 -0.0159687   0.00855509  0.0056773  -0.00530461]\n",
      "-------------\n",
      "Current epoch: 11, Running losses: 0.019746094194317033\n",
      "Current epoch: 11, Running Weights: [-0.01020596 -0.01596695  0.00854547  0.00567303 -0.00530441]\n",
      "-------------\n",
      "Current epoch: 12, Running losses: 0.019758386151184137\n",
      "Current epoch: 12, Running Weights: [-0.0102088  -0.01596519  0.00853585  0.00566876 -0.00530422]\n",
      "-------------\n",
      "Current epoch: 13, Running losses: 0.019770676080207657\n",
      "Current epoch: 13, Running Weights: [-0.01021164 -0.01596343  0.00852624  0.00566449 -0.00530402]\n",
      "-------------\n",
      "Current epoch: 14, Running losses: 0.01978296398168329\n",
      "Current epoch: 14, Running Weights: [-0.01021448 -0.01596167  0.00851662  0.00566022 -0.00530382]\n",
      "-------------\n",
      "Current epoch: 15, Running losses: 0.01979524985590608\n",
      "Current epoch: 15, Running Weights: [-0.01021731 -0.01595991  0.00850701  0.00565595 -0.00530362]\n",
      "-------------\n",
      "Current epoch: 16, Running losses: 0.019807533703172543\n",
      "Current epoch: 16, Running Weights: [-0.01022015 -0.01595816  0.00849739  0.00565168 -0.00530342]\n",
      "-------------\n",
      "Current epoch: 17, Running losses: 0.019819815523777536\n",
      "Current epoch: 17, Running Weights: [-0.01022298 -0.0159564   0.00848778  0.00564741 -0.00530323]\n",
      "-------------\n",
      "Current epoch: 18, Running losses: 0.019832095318016888\n",
      "Current epoch: 18, Running Weights: [-0.01022582 -0.01595464  0.00847816  0.00564314 -0.00530303]\n",
      "-------------\n",
      "Current epoch: 19, Running losses: 0.01984437308618578\n",
      "Current epoch: 19, Running Weights: [-0.01022865 -0.01595288  0.00846855  0.00563887 -0.00530283]\n",
      "-------------\n",
      "Current epoch: 20, Running losses: 0.019856648828580018\n",
      "Current epoch: 20, Running Weights: [-0.01023148 -0.01595112  0.00845894  0.0056346  -0.00530263]\n",
      "-------------\n",
      "Current epoch: 21, Running losses: 0.019868922545494796\n",
      "Current epoch: 21, Running Weights: [-0.01023431 -0.01594936  0.00844933  0.00563034 -0.00530243]\n",
      "-------------\n",
      "Current epoch: 22, Running losses: 0.019881194237225844\n",
      "Current epoch: 22, Running Weights: [-0.01023715 -0.01594759  0.00843972  0.00562607 -0.00530223]\n",
      "-------------\n",
      "Current epoch: 23, Running losses: 0.019893463904068077\n",
      "Current epoch: 23, Running Weights: [-0.01023998 -0.01594583  0.00843011  0.0056218  -0.00530203]\n",
      "-------------\n",
      "Current epoch: 24, Running losses: 0.019905731546317317\n",
      "Current epoch: 24, Running Weights: [-0.0102428  -0.01594407  0.0084205   0.00561753 -0.00530184]\n",
      "-------------\n",
      "Current epoch: 25, Running losses: 0.019917997164268758\n",
      "Current epoch: 25, Running Weights: [-0.01024563 -0.01594231  0.00841089  0.00561326 -0.00530164]\n",
      "-------------\n",
      "Current epoch: 26, Running losses: 0.019930260758217674\n",
      "Current epoch: 26, Running Weights: [-0.01024846 -0.01594054  0.00840128  0.005609   -0.00530144]\n",
      "-------------\n",
      "Current epoch: 27, Running losses: 0.019942522328459344\n",
      "Current epoch: 27, Running Weights: [-0.01025129 -0.01593878  0.00839167  0.00560473 -0.00530124]\n",
      "-------------\n",
      "Current epoch: 28, Running losses: 0.019954781875289106\n",
      "Current epoch: 28, Running Weights: [-0.01025411 -0.01593702  0.00838206  0.00560046 -0.00530104]\n",
      "-------------\n",
      "Current epoch: 29, Running losses: 0.019967039399002004\n",
      "Current epoch: 29, Running Weights: [-0.01025694 -0.01593525  0.00837245  0.0055962  -0.00530084]\n",
      "-------------\n",
      "Current epoch: 30, Running losses: 0.019979294899893422\n",
      "Current epoch: 30, Running Weights: [-0.01025976 -0.01593349  0.00836285  0.00559193 -0.00530064]\n",
      "-------------\n",
      "Current epoch: 31, Running losses: 0.019991548378258492\n",
      "Current epoch: 31, Running Weights: [-0.01026259 -0.01593173  0.00835324  0.00558766 -0.00530044]\n",
      "-------------\n",
      "Current epoch: 32, Running losses: 0.020003799834392186\n",
      "Current epoch: 32, Running Weights: [-0.01026541 -0.01592996  0.00834364  0.0055834  -0.00530024]\n",
      "-------------\n",
      "Current epoch: 33, Running losses: 0.020016049268590227\n",
      "Current epoch: 33, Running Weights: [-0.01026823 -0.0159282   0.00833403  0.00557913 -0.00530004]\n",
      "-------------\n",
      "Current epoch: 34, Running losses: 0.020028296681146897\n",
      "Current epoch: 34, Running Weights: [-0.01027106 -0.01592643  0.00832443  0.00557487 -0.00529984]\n",
      "-------------\n",
      "Current epoch: 35, Running losses: 0.02004054207235795\n",
      "Current epoch: 35, Running Weights: [-0.01027388 -0.01592466  0.00831482  0.0055706  -0.00529964]\n",
      "-------------\n",
      "Current epoch: 36, Running losses: 0.020052785442518054\n",
      "Current epoch: 36, Running Weights: [-0.0102767  -0.0159229   0.00830522  0.00556634 -0.00529944]\n",
      "-------------\n",
      "Current epoch: 37, Running losses: 0.020065026791922538\n",
      "Current epoch: 37, Running Weights: [-0.01027952 -0.01592113  0.00829562  0.00556207 -0.00529924]\n",
      "-------------\n",
      "Current epoch: 38, Running losses: 0.020077266120866105\n",
      "Current epoch: 38, Running Weights: [-0.01028233 -0.01591936  0.00828601  0.00555781 -0.00529904]\n",
      "-------------\n",
      "Current epoch: 39, Running losses: 0.02008950342964374\n",
      "Current epoch: 39, Running Weights: [-0.01028515 -0.01591759  0.00827641  0.00555354 -0.00529883]\n",
      "-------------\n",
      "Current epoch: 40, Running losses: 0.020101738718550487\n",
      "Current epoch: 40, Running Weights: [-0.01028797 -0.01591583  0.00826681  0.00554928 -0.00529863]\n",
      "-------------\n",
      "Current epoch: 41, Running losses: 0.020113971987881493\n",
      "Current epoch: 41, Running Weights: [-0.01029079 -0.01591406  0.00825721  0.00554501 -0.00529843]\n",
      "-------------\n",
      "Current epoch: 42, Running losses: 0.020126203237931253\n",
      "Current epoch: 42, Running Weights: [-0.0102936  -0.01591229  0.00824761  0.00554075 -0.00529823]\n",
      "-------------\n",
      "Current epoch: 43, Running losses: 0.02013843246899502\n",
      "Current epoch: 43, Running Weights: [-0.01029642 -0.01591052  0.00823801  0.00553648 -0.00529803]\n",
      "-------------\n",
      "Current epoch: 44, Running losses: 0.020150659681367235\n",
      "Current epoch: 44, Running Weights: [-0.01029923 -0.01590875  0.00822841  0.00553222 -0.00529783]\n",
      "-------------\n",
      "Current epoch: 45, Running losses: 0.02016288487534286\n",
      "Current epoch: 45, Running Weights: [-0.01030204 -0.01590698  0.00821881  0.00552796 -0.00529763]\n",
      "-------------\n",
      "Current epoch: 46, Running losses: 0.020175108051216736\n",
      "Current epoch: 46, Running Weights: [-0.01030485 -0.01590521  0.00820922  0.00552369 -0.00529743]\n",
      "-------------\n",
      "Current epoch: 47, Running losses: 0.0201873292092838\n",
      "Current epoch: 47, Running Weights: [-0.01030767 -0.01590344  0.00819962  0.00551943 -0.00529722]\n",
      "-------------\n",
      "Current epoch: 48, Running losses: 0.02019954834983837\n",
      "Current epoch: 48, Running Weights: [-0.01031048 -0.01590167  0.00819002  0.00551517 -0.00529702]\n",
      "-------------\n",
      "Current epoch: 49, Running losses: 0.020211765473175478\n",
      "Current epoch: 49, Running Weights: [-0.01031329 -0.0158999   0.00818043  0.0055109  -0.00529682]\n",
      "-------------\n",
      "Current epoch: 50, Running losses: 0.02022398057958983\n",
      "Current epoch: 50, Running Weights: [-0.0103161  -0.01589813  0.00817083  0.00550664 -0.00529662]\n",
      "-------------\n",
      "Current epoch: 51, Running losses: 0.020236193669376002\n",
      "Current epoch: 51, Running Weights: [-0.01031891 -0.01589635  0.00816124  0.00550238 -0.00529641]\n",
      "-------------\n",
      "Current epoch: 52, Running losses: 0.020248404742828408\n",
      "Current epoch: 52, Running Weights: [-0.01032171 -0.01589458  0.00815164  0.00549812 -0.00529621]\n",
      "-------------\n",
      "Current epoch: 53, Running losses: 0.020260613800241894\n",
      "Current epoch: 53, Running Weights: [-0.01032452 -0.01589281  0.00814205  0.00549386 -0.00529601]\n",
      "-------------\n",
      "Current epoch: 54, Running losses: 0.0202728208419111\n",
      "Current epoch: 54, Running Weights: [-0.01032733 -0.01589103  0.00813246  0.00548959 -0.00529581]\n",
      "-------------\n",
      "Current epoch: 55, Running losses: 0.02028502586813044\n",
      "Current epoch: 55, Running Weights: [-0.01033013 -0.01588926  0.00812286  0.00548533 -0.0052956 ]\n",
      "-------------\n",
      "Current epoch: 56, Running losses: 0.020297228879194384\n",
      "Current epoch: 56, Running Weights: [-0.01033294 -0.01588749  0.00811327  0.00548107 -0.0052954 ]\n",
      "-------------\n",
      "Current epoch: 57, Running losses: 0.020309429875397605\n",
      "Current epoch: 57, Running Weights: [-0.01033574 -0.01588571  0.00810368  0.00547681 -0.0052952 ]\n",
      "-------------\n",
      "Current epoch: 58, Running losses: 0.020321628857034528\n",
      "Current epoch: 58, Running Weights: [-0.01033854 -0.01588394  0.00809409  0.00547255 -0.00529499]\n",
      "-------------\n",
      "Current epoch: 59, Running losses: 0.020333825824399496\n",
      "Current epoch: 59, Running Weights: [-0.01034135 -0.01588216  0.0080845   0.00546829 -0.00529479]\n",
      "-------------\n",
      "Current epoch: 60, Running losses: 0.02034602077778698\n",
      "Current epoch: 60, Running Weights: [-0.01034415 -0.01588039  0.00807491  0.00546403 -0.00529459]\n",
      "-------------\n",
      "Current epoch: 61, Running losses: 0.020358213717491316\n",
      "Current epoch: 61, Running Weights: [-0.01034695 -0.01587861  0.00806532  0.00545977 -0.00529438]\n",
      "-------------\n",
      "Current epoch: 62, Running losses: 0.02037040464380702\n",
      "Current epoch: 62, Running Weights: [-0.01034975 -0.01587683  0.00805573  0.00545551 -0.00529418]\n",
      "-------------\n",
      "Current epoch: 63, Running losses: 0.020382593557028282\n",
      "Current epoch: 63, Running Weights: [-0.01035255 -0.01587506  0.00804614  0.00545125 -0.00529398]\n",
      "-------------\n",
      "Current epoch: 64, Running losses: 0.020394780457449496\n",
      "Current epoch: 64, Running Weights: [-0.01035535 -0.01587328  0.00803656  0.00544699 -0.00529377]\n",
      "-------------\n",
      "Current epoch: 65, Running losses: 0.020406965345365017\n",
      "Current epoch: 65, Running Weights: [-0.01035815 -0.0158715   0.00802697  0.00544273 -0.00529357]\n",
      "-------------\n",
      "Current epoch: 66, Running losses: 0.02041914822106915\n",
      "Current epoch: 66, Running Weights: [-0.01036094 -0.01586972  0.00801738  0.00543847 -0.00529336]\n",
      "-------------\n",
      "Current epoch: 67, Running losses: 0.020431329084855773\n",
      "Current epoch: 67, Running Weights: [-0.01036374 -0.01586795  0.0080078   0.00543421 -0.00529316]\n",
      "-------------\n",
      "Current epoch: 68, Running losses: 0.02044350793701931\n",
      "Current epoch: 68, Running Weights: [-0.01036653 -0.01586617  0.00799821  0.00542995 -0.00529296]\n",
      "-------------\n",
      "Current epoch: 69, Running losses: 0.020455684777854293\n",
      "Current epoch: 69, Running Weights: [-0.01036933 -0.01586439  0.00798863  0.00542569 -0.00529275]\n",
      "-------------\n",
      "Current epoch: 70, Running losses: 0.020467859607654355\n",
      "Current epoch: 70, Running Weights: [-0.01037212 -0.01586261  0.00797904  0.00542143 -0.00529255]\n",
      "-------------\n",
      "Current epoch: 71, Running losses: 0.020480032426713795\n",
      "Current epoch: 71, Running Weights: [-0.01037492 -0.01586083  0.00796946  0.00541718 -0.00529234]\n",
      "-------------\n",
      "Current epoch: 72, Running losses: 0.02049220323532663\n",
      "Current epoch: 72, Running Weights: [-0.01037771 -0.01585905  0.00795988  0.00541292 -0.00529214]\n",
      "-------------\n",
      "Current epoch: 73, Running losses: 0.020504372033787313\n",
      "Current epoch: 73, Running Weights: [-0.0103805  -0.01585727  0.0079503   0.00540866 -0.00529193]\n",
      "-------------\n",
      "Current epoch: 74, Running losses: 0.020516538822389442\n",
      "Current epoch: 74, Running Weights: [-0.01038329 -0.01585549  0.00794071  0.0054044  -0.00529173]\n",
      "-------------\n",
      "Current epoch: 75, Running losses: 0.02052870360142736\n",
      "Current epoch: 75, Running Weights: [-0.01038608 -0.01585371  0.00793113  0.00540014 -0.00529152]\n",
      "-------------\n",
      "Current epoch: 76, Running losses: 0.020540866371194805\n",
      "Current epoch: 76, Running Weights: [-0.01038887 -0.01585192  0.00792155  0.00539589 -0.00529132]\n",
      "-------------\n",
      "Current epoch: 77, Running losses: 0.02055302713198597\n",
      "Current epoch: 77, Running Weights: [-0.01039166 -0.01585014  0.00791197  0.00539163 -0.00529111]\n",
      "-------------\n",
      "Current epoch: 78, Running losses: 0.02056518588409466\n",
      "Current epoch: 78, Running Weights: [-0.01039445 -0.01584836  0.00790239  0.00538737 -0.0052909 ]\n",
      "-------------\n",
      "Current epoch: 79, Running losses: 0.020577342627814858\n",
      "Current epoch: 79, Running Weights: [-0.01039724 -0.01584658  0.00789281  0.00538312 -0.0052907 ]\n",
      "-------------\n",
      "Current epoch: 80, Running losses: 0.02058949736344049\n",
      "Current epoch: 80, Running Weights: [-0.01040002 -0.01584479  0.00788323  0.00537886 -0.00529049]\n",
      "-------------\n",
      "Current epoch: 81, Running losses: 0.020601650091265262\n",
      "Current epoch: 81, Running Weights: [-0.01040281 -0.01584301  0.00787366  0.0053746  -0.00529029]\n",
      "-------------\n",
      "Current epoch: 82, Running losses: 0.02061380081158318\n",
      "Current epoch: 82, Running Weights: [-0.01040559 -0.01584123  0.00786408  0.00537035 -0.00529008]\n",
      "-------------\n",
      "Current epoch: 83, Running losses: 0.020625949524687923\n",
      "Current epoch: 83, Running Weights: [-0.01040838 -0.01583944  0.0078545   0.00536609 -0.00528987]\n",
      "-------------\n",
      "Current epoch: 84, Running losses: 0.020638096230873364\n",
      "Current epoch: 84, Running Weights: [-0.01041116 -0.01583766  0.00784493  0.00536183 -0.00528967]\n",
      "-------------\n",
      "Current epoch: 85, Running losses: 0.020650240930433408\n",
      "Current epoch: 85, Running Weights: [-0.01041394 -0.01583587  0.00783535  0.00535758 -0.00528946]\n",
      "-------------\n",
      "Current epoch: 86, Running losses: 0.020662383623661396\n",
      "Current epoch: 86, Running Weights: [-0.01041672 -0.01583409  0.00782578  0.00535332 -0.00528926]\n",
      "-------------\n",
      "Current epoch: 87, Running losses: 0.020674524310851296\n",
      "Current epoch: 87, Running Weights: [-0.01041951 -0.0158323   0.0078162   0.00534907 -0.00528905]\n",
      "-------------\n",
      "Current epoch: 88, Running losses: 0.020686662992296854\n",
      "Current epoch: 88, Running Weights: [-0.01042229 -0.01583052  0.00780663  0.00534481 -0.00528884]\n",
      "-------------\n",
      "Current epoch: 89, Running losses: 0.0206987996682914\n",
      "Current epoch: 89, Running Weights: [-0.01042507 -0.01582873  0.00779705  0.00534056 -0.00528863]\n",
      "-------------\n",
      "Current epoch: 90, Running losses: 0.02071093433912898\n",
      "Current epoch: 90, Running Weights: [-0.01042784 -0.01582694  0.00778748  0.0053363  -0.00528843]\n",
      "-------------\n",
      "Current epoch: 91, Running losses: 0.020723067005102916\n",
      "Current epoch: 91, Running Weights: [-0.01043062 -0.01582516  0.00777791  0.00533205 -0.00528822]\n",
      "-------------\n",
      "Current epoch: 92, Running losses: 0.020735197666506888\n",
      "Current epoch: 92, Running Weights: [-0.0104334  -0.01582337  0.00776834  0.0053278  -0.00528801]\n",
      "-------------\n",
      "Current epoch: 93, Running losses: 0.020747326323634404\n",
      "Current epoch: 93, Running Weights: [-0.01043618 -0.01582158  0.00775877  0.00532354 -0.00528781]\n",
      "-------------\n",
      "Current epoch: 94, Running losses: 0.02075945297677871\n",
      "Current epoch: 94, Running Weights: [-0.01043895 -0.01581979  0.0077492   0.00531929 -0.0052876 ]\n",
      "-------------\n",
      "Current epoch: 95, Running losses: 0.02077157762623397\n",
      "Current epoch: 95, Running Weights: [-0.01044173 -0.015818    0.00773963  0.00531503 -0.00528739]\n",
      "-------------\n",
      "Current epoch: 96, Running losses: 0.020783700272292864\n",
      "Current epoch: 96, Running Weights: [-0.0104445  -0.01581621  0.00773006  0.00531078 -0.00528718]\n",
      "-------------\n",
      "Current epoch: 97, Running losses: 0.020795820915249504\n",
      "Current epoch: 97, Running Weights: [-0.01044728 -0.01581442  0.00772049  0.00530653 -0.00528697]\n",
      "-------------\n",
      "Current epoch: 98, Running losses: 0.020807939555396945\n",
      "Current epoch: 98, Running Weights: [-0.01045005 -0.01581263  0.00771092  0.00530227 -0.00528677]\n",
      "-------------\n",
      "Current epoch: 99, Running losses: 0.020820056193028503\n",
      "Current epoch: 99, Running Weights: [-0.01045282 -0.01581084  0.00770135  0.00529802 -0.00528656]\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01045282, -0.01581084,  0.00770135,  0.00529802, -0.00528656])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0000001\n",
    "epochs = 100\n",
    "\n",
    "result = train(X, W, y, epochs, lr)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e126234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X: ndarray, theta: ndarray, threshold: float) -> ndarray:\n",
    "    return np.where(thesis(theta, X) >= threshold, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee705ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = test(X[6], result, 0.6)\n",
    "predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
